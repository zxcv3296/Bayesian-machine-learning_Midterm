Gaussian Mixture Model (GMM) - EM / Gibbs / Variational Inference
==================================================================

ğŸ“˜ ê°œìš” (Overview)
------------------------------------------------------------------
**NumPyë§Œì„ ì‚¬ìš©í•˜ì—¬** Gaussian Mixture Model(GMM)ì„ ì„¸ ê°€ì§€ ì¶”ë¡  ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
1. **EM (Expectationâ€“Maximization)** â€” ìµœëŒ€ìš°ë„ ì¶”ì • ê¸°ë°˜ ì ì¶”ì • ë°©ì‹
2. **Gibbs Sampling** â€” ì™„ì „ ë² ì´ì§€ì•ˆ ì‚¬í›„ ìƒ˜í”Œë§
3. **Variational Inference (VI)** â€” ê·¼ì‚¬ ì¶”ë¡  (ELBO ìµœëŒ€í™” ê¸°ë°˜)

ëª¨ë“  ë°©ë²•ì€ ë™ì¼í•œ 2ì°¨ì› ë°ì´í„°ì…‹ `data/G2.txt` ë¥¼ ì‚¬ìš©í•˜ì—¬, K=1~4ì— ëŒ€í•´ ì‹¤í—˜í•˜ì˜€ìŠµë‹ˆë‹¤.


------------------------------------------------------------------
ğŸ§  ì ‘ê·¼ ë°©ì‹ (Approach)
------------------------------------------------------------------
ì„¸ ê°€ì§€ ì¶”ë¡  ê¸°ë²•ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### 1ï¸âƒ£ EM (Expectationâ€“Maximization)
- ì ì¬ë³€ìˆ˜ zì— ëŒ€í•œ **ê¸°ëŒ€ê°’(E-step)** ê³¼ íŒŒë¼ë¯¸í„°(Î¼, Î£, Ï€) ê°±ì‹ (**M-step**)ì„ ë°˜ë³µí•˜ë©° ë¡œê·¸ìš°ë„ë¥¼ ìµœëŒ€í™”.
- ë‹¨ìˆœí•˜ì§€ë§Œ ì´ˆê¸°ê°’ì— ë¯¼ê°í•˜ë©° ì§€ì—­ ìµœì ì (local optimum)ì— ë¹ ì§ˆ ìˆ˜ ìˆìŒ.

### 2ï¸âƒ£ Gibbs Sampling
- ë² ì´ì§€ì•ˆ ì¶”ë¡  ê¸°ë°˜ì˜ **MCMC(Markov Chain Monte Carlo)** ìƒ˜í”Œë§.
- ë§¤ ë°˜ë³µë§ˆë‹¤ ì ì¬ë³€ìˆ˜ z, í˜¼í•©ë¹„ìœ¨ Ï€, í‰ê·  Î¼, ê³µë¶„ì‚° Î£ë¥¼ ì‚¬í›„ë¶„í¬ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§.
- Burn-in ì´í›„ ìƒ˜í”Œì˜ í‰ê· ì„ ìµœì¢… ì¶”ì •ê°’ìœ¼ë¡œ ì‚¬ìš© â†’ ë¶ˆí™•ì‹¤ì„±ì„ ë°˜ì˜.

### 3ï¸âƒ£ Variational Inference (VI)
- ë³µì¡í•œ ì‚¬í›„ë¶„í¬ë¥¼ ë‹¨ìˆœí•œ ê·¼ì‚¬ ë¶„í¬(q)ë¡œ í‘œí˜„í•˜ê³ , **ELBO(Evidence Lower Bound)** ë¥¼ ìµœëŒ€í™”.
- ë¹ ë¥¸ ìˆ˜ë ´ì„±ê³¼ ì•ˆì •ì„±ì´ ì¥ì ì´ë©°, Gibbsë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì ì„.


------------------------------------------------------------------
âš™ï¸ ì‹¤í—˜ ì„¤ì • (Experimental Setup)
------------------------------------------------------------------
| í•­ëª© | ì„¤ì •ê°’ |
|------|---------|
| **Dataset** | `data/G2.txt` (2D, 2ê°œ í´ëŸ¬ìŠ¤í„°) |
| **K ê°’** | 1, 2, 3, 4 |
| **Seed** | 42 |
| **EM** | max_iter=200, tol=1e-6 |
| **Gibbs** | burnin=500, iters=2000, thin=5 |
| **VI** | max_iter=500, tol=1e-6 |
| **Metrics** | Log-Likelihood (LL), ELBO, AIC, BIC, Purity, ARI, NMI |
| **ì‹œê°í™” ê²°ê³¼** | ê° Kë³„ í´ëŸ¬ìŠ¤í„° ë„ì‹(`runs/`) + ìˆ˜ë ´ê³¡ì„  |


------------------------------------------------------------------
ğŸ“‚ í´ë” êµ¬ì¡° ë° ì£¼ìš” íŒŒì¼ ì„¤ëª…
------------------------------------------------------------------
project/  
â”œâ”€ **common/**  
â”‚   â”œâ”€ `io_utils.py` : ë°ì´í„° ë¡œë“œ, Z-score í‘œì¤€í™”  
â”‚   â”œâ”€ `metrics.py` : LL, AIC/BIC, Purity, ARI, NMI ê³„ì‚°  
â”‚   â”œâ”€ `utils.py` : ìˆ˜í•™ ìœ í‹¸ (logsumexp, mvn_logpdf ë“±)  
â”‚   â””â”€ `viz.py` : ì‚°ì ë„ + íƒ€ì›, ìˆ˜ë ´ê³¡ì„  ì‹œê°í™”  
â”‚  
â”œâ”€ **data/**  
â”‚   â”œâ”€ `G2.txt` : ì‹¤í—˜ìš© ë°ì´í„° (2D, 2ê°œ ëª¨ë“œ)  
â”‚   â”œâ”€ `g2-*.txt` : ë‹¤ë¥¸ ìƒ˜í”Œ ì„¸íŠ¸  
â”‚  
â”œâ”€ **em/** â†’ EM êµ¬í˜„  
â”‚   â””â”€ `gmm_em.py` : EM í•™ìŠµ ë° LL/BIC ê³„ì‚°  
â”‚  
â”œâ”€ **gibbs/** â†’ Gibbs Sampling êµ¬í˜„  
â”‚   â””â”€ `gmm_gibbs.py` : Dirichlet-NIW ê¸°ë°˜ ë² ì´ì§€ì•ˆ GMM  
â”‚  
â”œâ”€ **vi/** â†’ Variational Inference êµ¬í˜„  
â”‚   â””â”€ `gmm_vi.py` : Mean-field VI (ELBO ê³„ì‚° í¬í•¨)  
â”‚  
â”œâ”€ **runs/** : ì‹¤í–‰ ê²°ê³¼  
â”‚   â”œâ”€ `*_K*.png` : í´ëŸ¬ìŠ¤í„° ì‹œê°í™”  
â”‚   â”œâ”€ `*_ll_K*.png`, `vi_elbo_K*.png` : ìˆ˜ë ´ ê³¡ì„   
â”‚   â””â”€ `*.npz` : í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì €ì¥  
â”‚  
â””â”€ **env.yml** : Conda í™˜ê²½ ì„¤ì • íŒŒì¼  


------------------------------------------------------------------
ğŸš€ ì‹¤í–‰ ë° ì˜ˆìƒ ì¶œë ¥ (Environment & Expected Output)
------------------------------------------------------------------

1ï¸âƒ£ **í™˜ê²½ êµ¬ì„±**
```bash
conda env create -f env.yml
conda activate gmm-g2
```

2ï¸âƒ£ **EM ì‹¤í–‰ ì˜ˆì‹œ**
```bash
python em/gmm_em.py --data data/G2.txt --K 2 --max_iter 200 --tol 1e-6 --seed 42   --plot_out runs/em_K2.png --curve_out runs/em_ll_K2.png --save_params runs/em_K2.npz
```
**ì¶œë ¥**
```
[EM] K=2  LL=-22812.1478  AIC=45646.30  BIC=45708.17  iters=96
[EM] Purity=0.9170  ARI=0.6954  NMI=0.5873
```

3ï¸âƒ£ **Gibbs ì‹¤í–‰ ì˜ˆì‹œ**
```bash
python gibbs/gmm_gibbs.py --data data/G2.txt --K 2 --burnin 500 --iters 2000 --thin 5 --seed 42   --plot_out runs/gibbs_K2.png --save_params runs/gibbs_K2.npz
```
**ì¶œë ¥**
```
[Gibbs] K=2 samples_collected=400  LL_mean=-28613.974372661967
[Gibbs] Purity=0.9170  ARI=0.6954  NMI=0.5873
```

4ï¸âƒ£ **VI ì‹¤í–‰ ì˜ˆì‹œ**
```bash
python vi/gmm_vi.py --data data/G2.txt --K 2 --max_iter 500 --tol 1e-6 --seed 42   --plot_out runs/vi_K2.png --curve_out runs/vi_elbo_K2.png --save_params runs/vi_K2.npz
```
**ì¶œë ¥**
```
[VI] K=2  iters=17  ELBO=41068.0854
[VI] Purity=0.9170  ARI=0.6954  NMI=0.5873
```


------------------------------------------------------------------
ğŸ“Š ê²°ê³¼ ë° ë…¼ì˜ (Results & Discussion)
------------------------------------------------------------------

| Method | Score Type | Value | Purity | ARI | NMI | íŠ¹ì§• |
|--------|-------------|--------|--------|-----|-----|------|
| **EM** | LL | -22812 | 0.917 | 0.695 | 0.587 | ë¹ ë¥¸ ìˆ˜ë ´, ì•ˆì •ì  |
| **Gibbs** | LL_mean | -28610 | 0.917 | 0.695 | 0.587 | ë¶ˆí™•ì‹¤ì„± ë°˜ì˜, ëŠë¦¼ |
| **VI** | ELBO | 41068 | 0.917 | 0.695 | 0.587 | ë¹ ë¥´ê³  ì•ˆì •ì , ê·¼ì‚¬ ì˜¤ì°¨ ì¡´ì¬ |

**ë¶„ì„ ìš”ì•½**
- ì„¸ ë°©ë²• ëª¨ë‘ ë™ì¼í•œ êµ°ì§‘ êµ¬ì¡°(K=2)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë³µì›í•¨.
- EMì€ ë¹ ë¥´ì§€ë§Œ ì§€ì—­ ìµœì ì ì— ë¯¼ê°.  
- GibbsëŠ” ë¶ˆí™•ì‹¤ì„± ë°˜ì˜ì´ ê°€ëŠ¥í•˜ë‚˜ ê³„ì‚°ëŸ‰ì´ í¼.  
- VIëŠ” ë¹ ë¥¸ ìˆ˜ë ´ì„±ê³¼ ì•ˆì •ì„±ì„ ë™ì‹œì— ë‹¬ì„±.  
- BIC ê¸°ì¤€ìœ¼ë¡œ K=2ê°€ ìµœì  ëª¨ë¸ë¡œ íŒë‹¨ë¨.

ì‹œê°ì ìœ¼ë¡œ `runs/` í´ë” ë‚´ ì´ë¯¸ì§€(`em_K2.png`, `vi_K2.png`, `gibbs_K2.png`)ì—ì„œ
ëª¨ë‘ ë™ì¼í•œ ë‘ ê°œì˜ íƒ€ì›í˜• í´ëŸ¬ìŠ¤í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

